{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-11T03:35:31.222857Z","iopub.execute_input":"2022-01-11T03:35:31.223202Z","iopub.status.idle":"2022-01-11T03:35:34.883709Z","shell.execute_reply.started":"2022-01-11T03:35:31.223169Z","shell.execute_reply":"2022-01-11T03:35:34.882734Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hello every-one , I am sharing the basic approach to tackle this competition. This code is for beginners who are new to kaggle , pytorch and data-science. I have tried to explain each and every code cell in a layman terms so that many beginners who get confused to understand the code of high scoring notebooks can easily get what I have done here.\n**This notebook scored 18.50 on public leaderboad(pb) and 17.87 during validation**","metadata":{}},{"cell_type":"markdown","source":"# Basic Imports","metadata":{}},{"cell_type":"code","source":"import math #for mathematical operations\nimport time \nimport os\nfrom skimage import io, transform #for augmentation purposes\nimport PIL #for image processing\nworkers=2\nworking_dir = './'#for loading saved files to this location , will be used at the end","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:34.886501Z","iopub.execute_input":"2022-01-11T03:35:34.886895Z","iopub.status.idle":"2022-01-11T03:35:34.892226Z","shell.execute_reply.started":"2022-01-11T03:35:34.886844Z","shell.execute_reply":"2022-01-11T03:35:34.891132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONFIG","metadata":{}},{"cell_type":"markdown","source":"No need to get confused, I am just using names instead of copying the link of the directory","metadata":{}},{"cell_type":"code","source":"main_dir = '../input/petfinder-pawpularity-score' \nbatch_size = 32\nnp.random.seed(100)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:34.893844Z","iopub.execute_input":"2022-01-11T03:35:34.894152Z","iopub.status.idle":"2022-01-11T03:35:34.904077Z","shell.execute_reply.started":"2022-01-11T03:35:34.89411Z","shell.execute_reply":"2022-01-11T03:35:34.903047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt # used for plotting\nimport torch #pytorch\nfrom torchvision import datasets,transforms, models \nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts #scheduler used for finding global minima\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler","metadata":{"execution":{"iopub.status.busy":"2022-01-11T06:55:35.163999Z","iopub.execute_input":"2022-01-11T06:55:35.164669Z","iopub.status.idle":"2022-01-11T06:55:35.171541Z","shell.execute_reply.started":"2022-01-11T06:55:35.164619Z","shell.execute_reply":"2022-01-11T06:55:35.170558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"train_df= pd.read_csv(f'{main_dir}/train.csv')  \ntrain_df.head(4)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:34.919782Z","iopub.execute_input":"2022-01-11T03:35:34.920554Z","iopub.status.idle":"2022-01-11T03:35:34.955095Z","shell.execute_reply.started":"2022-01-11T03:35:34.920503Z","shell.execute_reply":"2022-01-11T03:35:34.954074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:34.956687Z","iopub.execute_input":"2022-01-11T03:35:34.957391Z","iopub.status.idle":"2022-01-11T03:35:34.976261Z","shell.execute_reply.started":"2022-01-11T03:35:34.957329Z","shell.execute_reply":"2022-01-11T03:35:34.975066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_df.iloc[:,13],bins=50,facecolor ='r',alpha= 0.5)\nplt.xlabel('pawpularity_score')\nplt.ylabel('frequency')\nplt.title('PAWPULARITY DISTRIBUTION')\nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:34.977833Z","iopub.execute_input":"2022-01-11T03:35:34.978556Z","iopub.status.idle":"2022-01-11T03:35:35.374674Z","shell.execute_reply.started":"2022-01-11T03:35:34.97851Z","shell.execute_reply":"2022-01-11T03:35:35.37353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now comes the tricky part( not that tricky though!!). I have used basic OOPs , this way code looks clean and its easy to track the info.\n* The code below , gives outthe pawpularity score , transformed image and annotations. This class so formed will be called in further code cells... and it consits of 3 methods __init__, __len__,__getitem__\n* Basically this compitition deals with images and tabular data, so we have to merge both type of data .... in __get_item__ method, the path of the images have been added into the table of annotations according to image ids. In this manner Pawpularity score act as a label/ answers(supervised learning) for the image.\n\n","metadata":{}},{"cell_type":"code","source":"class Pawpularity_Data(Dataset):\n    def __init__(self,csv_file,img_dir,transform = transforms.ToTensor()):\n        self.annotations_csv = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform =transform\n        \n    def __len__(self):\n        return len(self.annotations_csv)\n    \n    def __getitem__(self,idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        img_name = os.path.join(self.img_dir,self.annotations_csv.iloc[idx,0])\n        image = PIL.Image.open(img_name + '.jpg')\n        # Columns 1 to 12 contain the annotations\n        annotations = np.array(self.annotations_csv.iloc[idx, 1:13])\n        annotations = annotations.astype('float')\n        # Column 13 has the scores\n        score = np.array(self.annotations_csv.iloc[idx, 13])\n        score = torch.tensor(score.astype('float')).view(1).to(torch.float32)\n        # Apply the transforms\n        image = self.transform(image)\n        sample = [image, annotations, score]\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:35.376569Z","iopub.execute_input":"2022-01-11T03:35:35.376933Z","iopub.status.idle":"2022-01-11T03:35:35.387521Z","shell.execute_reply.started":"2022-01-11T03:35:35.376887Z","shell.execute_reply":"2022-01-11T03:35:35.386262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Augmentations","metadata":{}},{"cell_type":"markdown","source":"* Image augmentation is done to deliberately increase the size of the dataset so that model gets trained on a large dataset, this makes models's prediction more robust\n* Augmentations can be of many types but according to my testings..... this for this competition , flipping and rotation gave better results \n* Whereas augmentations like blurring, changing contrast and vertical flipping hampered the prediction","metadata":{}},{"cell_type":"code","source":"# Test out the transforms on an image (images need to be made the same size for the dataset to work)\nimg_transforms = transforms.Compose([transforms.Resize(255),\n                                     transforms.CenterCrop(224),\n                                     transforms.RandomHorizontalFlip(),\n                                     transforms.RandomRotation(20),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                          std=[0.229, 0.224, 0.225])])\n\nimg_transforms_valid = transforms.Compose([transforms.Resize(255),\n                                           transforms.CenterCrop(224),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                                std=[0.229, 0.224, 0.225])])","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:35.38946Z","iopub.execute_input":"2022-01-11T03:35:35.390107Z","iopub.status.idle":"2022-01-11T03:35:35.404505Z","shell.execute_reply.started":"2022-01-11T03:35:35.390059Z","shell.execute_reply":"2022-01-11T03:35:35.403477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calling the class that we created earlier ","metadata":{}},{"cell_type":"code","source":"train_data = Pawpularity_Data(f'{main_dir}/train.csv',f'{main_dir}/train',transform =img_transforms)\ntrain_data.img_dir","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:35.406311Z","iopub.execute_input":"2022-01-11T03:35:35.40696Z","iopub.status.idle":"2022-01-11T03:35:35.435611Z","shell.execute_reply.started":"2022-01-11T03:35:35.406915Z","shell.execute_reply":"2022-01-11T03:35:35.434768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:35.437243Z","iopub.execute_input":"2022-01-11T03:35:35.438006Z","iopub.status.idle":"2022-01-11T03:35:35.443763Z","shell.execute_reply.started":"2022-01-11T03:35:35.437961Z","shell.execute_reply":"2022-01-11T03:35:35.442579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, annotations, scores = next(iter(dataloader))\nprint(images.shape)\nprint(scores.shape)\nprint(annotations.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:35.44543Z","iopub.execute_input":"2022-01-11T03:35:35.446064Z","iopub.status.idle":"2022-01-11T03:35:35.626104Z","shell.execute_reply.started":"2022-01-11T03:35:35.446019Z","shell.execute_reply":"2022-01-11T03:35:35.625092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOOKING AT IMAGES FROM DATA","metadata":{}},{"cell_type":"code","source":"def de_norm(tensor):\n    image = tensor.to('cpu').clone().detach()\n    image = image.numpy().squeeze()\n    image = image * np.array((0.229, 0.224, 0.225)).reshape(3, 1, 1) + np.array((0.485, 0.456, 0.406)).reshape(3, 1, 1)\n    img = (image * 255).astype(np.uint8) # unnormalize\n    return plt.imshow(np.transpose(img, (1, 2, 0)))","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:35.627998Z","iopub.execute_input":"2022-01-11T03:35:35.628543Z","iopub.status.idle":"2022-01-11T03:35:35.637648Z","shell.execute_reply.started":"2022-01-11T03:35:35.628495Z","shell.execute_reply":"2022-01-11T03:35:35.636512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im_numpy = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(20, 10))\n# display 20 images\nfor idx in np.arange(8):\n    ax = fig.add_subplot(2, 4, idx+1, xticks=[], yticks=[])\n    de_norm(images[idx])\n    ax.set_title(scores[idx].item())\n","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:35.64373Z","iopub.execute_input":"2022-01-11T03:35:35.644032Z","iopub.status.idle":"2022-01-11T03:35:36.539155Z","shell.execute_reply.started":"2022-01-11T03:35:35.643989Z","shell.execute_reply":"2022-01-11T03:35:36.534374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN MODEL ARCHITECTURE","metadata":{}},{"cell_type":"markdown","source":"BASIC IMPORTS","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as func\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:36.540828Z","iopub.execute_input":"2022-01-11T03:35:36.541696Z","iopub.status.idle":"2022-01-11T03:35:36.547234Z","shell.execute_reply.started":"2022-01-11T03:35:36.541652Z","shell.execute_reply":"2022-01-11T03:35:36.546421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the dense layer input size\n# Padding of 1 and of 3 means no change in the image dimensions apart from pooling\n\nsdim = 224/2/2/2/2/2 #maxpoolin layers reduce xy dimensions by 2\nprint(sdim)\nprint(sdim*sdim*256+12) # add in the annotations","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:36.548485Z","iopub.execute_input":"2022-01-11T03:35:36.549463Z","iopub.status.idle":"2022-01-11T03:35:36.563679Z","shell.execute_reply.started":"2022-01-11T03:35:36.549404Z","shell.execute_reply":"2022-01-11T03:35:36.562653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Important!!! dont skip this\nSince this competition needs , kaggle internet to be switched off before submitting the predictions, common issue may arise when importing a pretrained model like..... this .... model = models.resnet50(pretrained=True)....this approach is not wrong but it will require internet to download imagenet weights... which will throw an error during submission since internet is off.\n\nTo deal with this .... open a separate notebook and load a model like mentioned above and then save the model in .pt file and download it and upload in your working notebook.","metadata":{}},{"cell_type":"code","source":"model = torch.load('../input/resnet-pretrained/resnet_pretrained.pt')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:36.565566Z","iopub.execute_input":"2022-01-11T03:35:36.566695Z","iopub.status.idle":"2022-01-11T03:35:36.700234Z","shell.execute_reply.started":"2022-01-11T03:35:36.566638Z","shell.execute_reply":"2022-01-11T03:35:36.698919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classifier Head \nYou can use any classifier but I have gone with ANN with 5 dense layers..","metadata":{}},{"cell_type":"code","source":"# Disable gradients on all model parameters to freeze the weights\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.fc = nn.Sequential(nn.Linear(2048, 1024),\n                         nn.ReLU(),\n                         nn.Linear(1024, 512),\n                         nn.ReLU(),\n                         nn.Linear(512, 256),\n                         nn.ReLU(),\n                         nn.Linear(256, 128),\n                         nn.ReLU(),\n                         nn.Linear(128, 1),\n                         nn.Sigmoid())\n\nfor param in model.fc.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:36.701872Z","iopub.execute_input":"2022-01-11T03:35:36.702256Z","iopub.status.idle":"2022-01-11T03:35:36.763938Z","shell.execute_reply.started":"2022-01-11T03:35:36.702212Z","shell.execute_reply":"2022-01-11T03:35:36.762524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:36.766298Z","iopub.execute_input":"2022-01-11T03:35:36.766889Z","iopub.status.idle":"2022-01-11T03:35:36.7771Z","shell.execute_reply.started":"2022-01-11T03:35:36.766847Z","shell.execute_reply":"2022-01-11T03:35:36.775881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(40)\n\ncriterion = nn.MSELoss(reduction='sum')\n\n\n#Adam with L2 regularization\noptimizer = optim.AdamW(model.parameters(), lr=0.0008, weight_decay=0.2)\n\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,10,\n    T_mult=1,\n    eta_min=0,\n    last_epoch=-1,\n    verbose=False,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:36.779688Z","iopub.execute_input":"2022-01-11T03:35:36.780303Z","iopub.status.idle":"2022-01-11T03:35:36.796091Z","shell.execute_reply.started":"2022-01-11T03:35:36.780257Z","shell.execute_reply":"2022-01-11T03:35:36.794654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a small batch to test out the forward pass\n\ntrain_dataset = Pawpularity_Data(f'{main_dir}/train.csv', f'{main_dir}/train', transform=img_transforms)\ndataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nimages, annotations, scores = next(iter(dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:36.798321Z","iopub.execute_input":"2022-01-11T03:35:36.79919Z","iopub.status.idle":"2022-01-11T03:35:37.640013Z","shell.execute_reply.started":"2022-01-11T03:35:36.79912Z","shell.execute_reply":"2022-01-11T03:35:37.639005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test out the forward pass on a single batch\n# RMSE before any training (with random parameters): \nwith torch.no_grad():\n    train_loss = 0.0\n    output = model(images)*100\n    loss = criterion(output, scores)\n    math.sqrt(loss.item()/64)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:37.641508Z","iopub.execute_input":"2022-01-11T03:35:37.64188Z","iopub.status.idle":"2022-01-11T03:35:46.684858Z","shell.execute_reply.started":"2022-01-11T03:35:37.641836Z","shell.execute_reply":"2022-01-11T03:35:46.683841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(scores.dtype)\nprint(output.dtype)\nprint(torch.mean(output))\nprint(torch.std(output))","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:46.686236Z","iopub.execute_input":"2022-01-11T03:35:46.686725Z","iopub.status.idle":"2022-01-11T03:35:46.717263Z","shell.execute_reply.started":"2022-01-11T03:35:46.686683Z","shell.execute_reply":"2022-01-11T03:35:46.716055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load and set up the final training and validation dataset (use different transforms)\n\ntrain_data = Pawpularity_Data(f'{main_dir}/train.csv', f'{main_dir}/train', transform=img_transforms)\nvalid_data = Pawpularity_Data(f'{main_dir}/train.csv', f'{main_dir}/train', transform=img_transforms_valid)\n\nnp.random.seed(100)\n\n# obtain random indices that will be used for traingin/validation split\nvalid_size = 0.2\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n                                           sampler=train_sampler, num_workers=workers,\n                                           pin_memory=True) \nvalid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,\n                                           sampler=valid_sampler, num_workers=workers,\n                                           pin_memory=True) \n\nprint(len(train_loader)*batch_size)\nprint(len(valid_loader)*batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:46.719327Z","iopub.execute_input":"2022-01-11T03:35:46.719701Z","iopub.status.idle":"2022-01-11T03:35:46.768728Z","shell.execute_reply.started":"2022-01-11T03:35:46.719654Z","shell.execute_reply":"2022-01-11T03:35:46.767673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\ndevice = torch.cuda.get_device_name()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print(f'CUDA is available!  Training on GPU {device}...')","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:46.770298Z","iopub.execute_input":"2022-01-11T03:35:46.771288Z","iopub.status.idle":"2022-01-11T03:35:46.829528Z","shell.execute_reply.started":"2022-01-11T03:35:46.771241Z","shell.execute_reply":"2022-01-11T03:35:46.828573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING THE MODEL","metadata":{}},{"cell_type":"code","source":"# number of epochs to train the model\n# Use 40 epochs\n\nif train_on_gpu:\n    model.cuda()\n\nn_epochs = 70\n\nvalid_loss_min = np.Inf # track change in validation loss\n\ntrain_losses, valid_losses = [], []\n\nfor epoch in range(1, n_epochs+1):\n    \n    start = time.time()\n    current_lr = scheduler.get_last_lr()[0]\n    \n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    # put in training mode (enable dropout)\n    model.train()\n    for images, annotations, scores in train_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        # the annotations get added in the dense layers\n        output = model(images)*100\n        # print(output.dtype)\n        # print(scores.dtype)\n        # calculate the batch loss\n        loss = criterion(output, scores)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()\n        \n    ######################    \n    # validate the model #\n    ######################\n    # eval mode (no dropout)\n    model.eval()\n    with torch.no_grad():\n        for images, annotations, scores in valid_loader:\n            # move tensors to GPU if CUDA is available\n            if train_on_gpu:\n                images, annotations, scores = images.cuda(), annotations.cuda(), scores.cuda()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(images)*100\n            # calculate the batch loss\n            loss = criterion(output, scores)\n            # update average validation loss \n            valid_loss += loss.item()\n    \n    # calculate RMSE\n    train_loss = math.sqrt(train_loss/len(train_loader.sampler))\n    valid_loss = math.sqrt(valid_loss/len(valid_loader.sampler))\n    \n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n        \n    # increment learning rate decay\n    scheduler.step()\n    \n    # print training/validation statistics \n    # print(f'Epoch: {e}, {float(time.time() - start):.3f} seconds, lr={optimizer.lr}')\n    print('Epoch: {}, time: {:.3f}s, lr: {:.6f} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, float(time.time() - start), current_lr, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), f'{working_dir}pawpularity_best_model.pt')\n        valid_loss_min = valid_loss    ","metadata":{"execution":{"iopub.status.busy":"2022-01-11T03:35:46.832076Z","iopub.execute_input":"2022-01-11T03:35:46.832964Z","iopub.status.idle":"2022-01-11T06:55:34.66852Z","shell.execute_reply.started":"2022-01-11T03:35:46.832899Z","shell.execute_reply":"2022-01-11T06:55:34.667434Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best performing model on the validation set\nmodel.load_state_dict(torch.load(f'{working_dir}pawpularity_best_model.pt'))","metadata":{"execution":{"iopub.status.busy":"2022-01-11T06:55:34.670404Z","iopub.execute_input":"2022-01-11T06:55:34.671084Z","iopub.status.idle":"2022-01-11T06:55:34.80487Z","shell.execute_reply.started":"2022-01-11T06:55:34.671025Z","shell.execute_reply":"2022-01-11T06:55:34.803657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PawpularityTestDataset(Dataset):\n    \"\"\"Dataset connecting dog images to the score and annotations\"\"\"\n\n    def __init__(self, csv_file, img_dir, transform=transforms.ToTensor()):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            img_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n\n        self.annotations_csv = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations_csv)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.img_dir,\n                                self.annotations_csv.iloc[idx, 0])\n\n        # load each image in PIL format for compatibility with transforms\n        image = PIL.Image.open(img_name + '.jpg')\n\n        annotations = np.array(self.annotations_csv.iloc[idx, 1:13])\n        annotations = annotations.astype('float')\n\n        # Apply the transforms\n        image = self.transform(image)\n\n        sample = [image, annotations]\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-01-11T06:55:34.806322Z","iopub.execute_input":"2022-01-11T06:55:34.807161Z","iopub.status.idle":"2022-01-11T06:55:34.818658Z","shell.execute_reply.started":"2022-01-11T06:55:34.807114Z","shell.execute_reply":"2022-01-11T06:55:34.817437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load the test dataset\nimg_transforms = transforms.Compose([transforms.Resize(255),\n                                       transforms.CenterCrop(224),\n                                       transforms.ToTensor()])\n\ntest_data = PawpularityTestDataset(f'{main_dir}/test.csv', f'{main_dir}/test', transform=img_transforms_valid)\n\nbatch_size = min(len(test_data), 32)\n\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=workers) ","metadata":{"execution":{"iopub.status.busy":"2022-01-11T06:55:34.820675Z","iopub.execute_input":"2022-01-11T06:55:34.821561Z","iopub.status.idle":"2022-01-11T06:55:34.843581Z","shell.execute_reply.started":"2022-01-11T06:55:34.82151Z","shell.execute_reply":"2022-01-11T06:55:34.842615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(f'{main_dir}/test.csv')\ntest_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T06:55:34.846754Z","iopub.execute_input":"2022-01-11T06:55:34.847773Z","iopub.status.idle":"2022-01-11T06:55:34.87107Z","shell.execute_reply.started":"2022-01-11T06:55:34.847723Z","shell.execute_reply":"2022-01-11T06:55:34.870163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step through with a reasonable batch size and build up the output dataset\n\nmodel.eval()\noutputs = []\nfor images, annotations in test_loader:\n    # move tensors to GPU if CUDA is available\n    if train_on_gpu:\n        images, annotations = images.cuda(), annotations.cuda()\n    test_output = model(images)*100\n    outputs.extend(list(test_output.cpu().detach().numpy().reshape(len(test_output),)))\n    \nimg_names = list( test_df.iloc[:, 0].values)\noutputs = [round(x, 2) for x in outputs]\n\noutput_df = pd.DataFrame({'Id': img_names, 'Pawpularity': outputs})\noutput_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T06:55:34.872952Z","iopub.execute_input":"2022-01-11T06:55:34.875751Z","iopub.status.idle":"2022-01-11T06:55:35.14552Z","shell.execute_reply.started":"2022-01-11T06:55:34.875702Z","shell.execute_reply":"2022-01-11T06:55:35.14456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Write the output in the required format\noutput_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T06:55:35.148377Z","iopub.execute_input":"2022-01-11T06:55:35.14903Z","iopub.status.idle":"2022-01-11T06:55:35.157791Z","shell.execute_reply.started":"2022-01-11T06:55:35.148981Z","shell.execute_reply":"2022-01-11T06:55:35.156764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}